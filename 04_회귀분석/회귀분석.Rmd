---
title: "Regression"
athor: "hyo"
output: html_document
---

## 5. Econometrics with R project 
### 5.1 단순회귀분석          
#### 5.1.1 회귀분석의 기초 개념

* 회귀분석이 연속형 변수들에 대해 독립 변수와 종속 변수 사이의 상관관계를 나타내는 것이라면, 단순 회귀 분석은 독립 변수가 단일개일 때의 분석을 의미한다 
* 기본적인 회귀모형은, 
$$y_{i}  = \beta_0 + \beta_1X_{i} + e_{i}$$



```{r}
library(AER)
library(MASS)
```
```{r}
STR <- c(15, 17, 19, 20, 22, 23.5, 25)
TestScore <- c(680, 640, 670, 660, 630, 660, 635) 

# Print out sample data
STR
TestScore
```

##### 위 데이터를 활용하여 산점도를 만들기
* abline(a,b) : 절편이 a, 기울기가 b인 직선 추가
```{r}
plot(TestScore ~ STR)
abline(a = 713, b = -3)
```

##### Use CASchools data 
```{r}
data(CASchools)
head(CASchools)
```

* STR, TestScore 파생변수 생성
```{r}
CASchools$STR <- CASchools$students/CASchools$teachers 
CASchools$score <- (CASchools$read + CASchools$math)/2     
```

* STR, score 평균, 표준편차 계산하기
```{r}
avg_STR <- mean(CASchools$STR) 
avg_score <- mean(CASchools$score)

sd_STR <- sd(CASchools$STR) 
sd_score <- sd(CASchools$score)
```

* quantile(data,quantiles) : data를 quantiles 기준으로 나누어줌  
```{r}
quantiles <- c(0.10, 0.25, 0.4, 0.5, 0.6, 0.75, 0.9)
quant_STR <- quantile(CASchools$STR, quantiles)
quant_score <- quantile(CASchools$score, quantiles)
```


```{r}
# gather everything in a data.frame 
DistributionSummary <- data.frame(Average = c(avg_STR, avg_score), 
                                  StandardDeviation = c(sd_STR, sd_score), 
                                  quantile = rbind(quant_STR, quant_score))
```

```{r}
DistributionSummary
```

* TestScore, STR에 대한 산점도
  + 산점도를 살펴 볼 때 음의 상관관계를 미세하게 확인 할 수 있음
```{r}
plot(score ~ STR, 
     data = CASchools,
     main = "Scatterplot of TestScore and STR", 
     xlab = "STR (X)",
     ylab = "Test Score (Y)")
```

* TestScore, STR 상관관계
  + cor(): `+`일경우 양의 상관관계, `-`일경우 음의 상관관계, 0에 가까울 수록 서로 독립이다
```{r}
cor(CASchools$STR, CASchools$score)
```

#### 5.1.2 단순선형 회귀모형
* estimate the model and assign the result to linear_model
  + $score = 698.93 -2.28*STR$
  + STR이 한단위 증가함에 따라 score은 2.28만큼 감소한다
  
```{r}
linear_model <- lm(score ~ STR, data = CASchools)
linear_model
```

##### plot the data
```{r}
plot(score ~ STR, 
     data = CASchools,
     main = "Scatterplot of TestScore and STR", 
     xlab = "STR (X)",
     ylab = "Test Score (Y)",
     xlim = c(10, 30),
     ylim = c(600, 720))

## add the regression line
abline(linear_model) 
```

#### 5.1.3 회귀계수의 추정
* beta_1_hat, beta_0_hat 구하기
```{r}
attach(CASchools) 

beta_1 <- sum((STR - mean(STR)) * (score - mean(score))) / sum((STR - mean(STR))^2)
beta_0 <- mean(score) - beta_1 * mean(STR)

beta_1
beta_0
```

* p-value를 확인하면 매우 작은 값으로 `***`로 나타나며 유의하다고 말 할 수 있다
  + `**` : <0.001
  + `*`  : <0.05
  + `.`  : <0.1  
* R-squared:  결정계수
  + 회귀식의 적합도를 재는 척도
* Adjusted R-squared:  수정된 결정계수
  + 회귀식의 적합도를 재는 척도 다중회귀분석 시 변수가 추가 됨에 따라 증가하는 r-squared를 보정해준다
  
* F-statistic: 22.58 on 1 and 418 DF,  p-value: 2.783e-06
  + F 통계량을 통해 회귀식의 유의성을 판단 
  
```{r}
mod_summary <- summary(linear_model)
mod_summary
```

* set.seed() : 난수를 고정 시켜 고정된 값이 나오게끔한다
* lm()       : 회귀모델 추정 함수 
* predict()  : 추정된 회귀식을 적용시켜 종속변수를 예측 

```{r}
set.seed(321)
X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 5)  

Y <- X^2 + 2 * X + u                
mod_simple <- lm(Y ~ X)

prediction <- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X)))

# plot the results
plot(Y ~ X)
abline(mod_simple, col = "red")
lines(sort(X), prediction,col="blue")
```

##### 이상값
* outlier : 통계적 자료분석의 결과를 왜곡시키거나, 자료 분석의 적절성을 위협하는 변수값 또는 사례를 말한다
  + outlier에 의해 회귀분석의 결과가 심하게 달라질 수 있음
```{r}
set.seed(123)

# generate the data
X <- sort(runif(10, min = 30, max = 70))
Y <- rnorm(10 , mean = 200, sd = 50)
Y[9] <- 2000

# fit model with outlier
fit <- lm(Y ~ X)

# fit model without outlier
fitWithoutOutlier <- lm(Y[-9] ~ X[-9])

# plot the results
plot(Y ~ X)
abline(fit)
abline(fitWithoutOutlier, col = "red")
```


##### 실습1
* 임의의 데이터 bvndata를 만들어 평균 기준으로 데이터를 나눈뒤 회귀분석을 실시해보자

```{r}
# load the MASS package
library(MASS)

# set seed for reproducibility
set.seed(4)

# simulate bivarite normal data
bvndata <- mvrnorm(100, 
                mu = c(5, 5), 
                Sigma = cbind(c(5, 4), c(4, 5))) 

# assign column names / convert to data.frame
colnames(bvndata) <- c("X", "Y")
bvndata <- as.data.frame(bvndata)

# subset the data
set1 <- subset(bvndata, abs(mean(X) - X) > 1)
set2 <- subset(bvndata, abs(mean(X) - X) <= 1)

# plot both data sets
plot(set1, 
     xlab = "X", 
     ylab = "Y", 
     pch = 19)

points(set2, 
       col = "steelblue", 
       pch = 19)
```

##### plot observations
```{r}
# estimate both regression lines
lm.set1 <- lm(Y ~ X, data = set1)
lm.set2 <- lm(Y ~ X, data = set2)


plot(set1, xlab = "X", ylab = "Y", pch = 19)
points(set2, col = "steelblue", pch = 19)

abline(lm.set1, col = "green")
abline(lm.set2, col = "red")
```

#### 5.1.4 회귀직선의 적합도
##### 회귀직선의 적합도
* 가정된 회귀직선을 추정한 다음에는 그 회귀식이 얼마나 타당한가를 조사하여야 한다. 이를테면, 회귀분석의 목적은 종속변량을 독립변량의 함수로 설명하고자 함이므로 과연 그 설명의 정도가 어느 정도인지를 알아 볼 필요가 있다. 이와 같은 타당성 조사에는 잔차표준오차(residual standard error)와 결정계수(coefficient ofdetermination)가 사용된다
```{r}
library(AER)
library(scales)
```

* summary()$coefficients을 할 경우 회귀계수 추정값에 대한 정보만을 보여준다

```{r}
# Testing Two-Sided Hypotheses Concerning the Slope Coefficient
# load the `CASchools` dataset
data(CASchools)

# add student-teacher ratio
CASchools$STR <- CASchools$students/CASchools$teachers

# add average test-score
CASchools$score <- (CASchools$read + CASchools$math)/2

# estimate the model
linear_model <- lm(score ~ STR, data = CASchools)          
```

* 귀무가설 : 독립변수와 종속변수간의 유의한 상관관계가 존재하지않는다
* 대립가설 : 독립변수와 종속변수간의 유의한 상관관계가 존재한다
  + p-value값을 살펴보면 < 0.05이므로 유의수준 5%하에서 귀무가설을 기각할만한 충분한 근거가된다

```{r}
# print the summary of the coefficients to the console
summary(linear_model)$coefficients
```

* Residual standard error: 18.58
* Adjusted R-squared:  0.04897 
```{r}
summary(linear_model)
```

#### 5.1.5 단순회귀 분석에서의 추론
##### T분포
* p-value < 0.05 기준으로 귀무가설을 기각할 확률은 아래 보이는 그래프의 색칠된 부분이다 
```{r}
# Plot the standard normal on the support [-6,6]
t <- seq(-6, 6, 0.01)

plot(x = t, 
     y = dnorm(t, 0, 1), 
     type = "l", 
     col = "steelblue", 
     lwd = 2, 
     yaxs = "i", 
     axes = F, 
     ylab = "", 
     main = expression("Calculating the p-value of a Two-sided Test when" ~ t^act ~ "=-0.47"), 
     cex.lab = 0.7,
     cex.main = 1)

tact <- -4.75

axis(1, at = c(0, -1.96, 1.96, -tact, tact), cex.axis = 0.7)

# Shade the critical regions using polygon():

# critical region in left tail
polygon(x = c(-6, seq(-6, -1.96, 0.01), -1.96),
        y = c(0, dnorm(seq(-6, -1.96, 0.01)), 0), 
        col = 'orange')

# critical region in right tail

polygon(x = c(1.96, seq(1.96, 6, 0.01), 6),
        y = c(0, dnorm(seq(1.96, 6, 0.01)), 0), 
        col = 'orange')

# Add arrows and texts indicating critical regions and the p-value
arrows(-3.5, 0.2, -2.5, 0.02, length = 0.1)
arrows(3.5, 0.2, 2.5, 0.02, length = 0.1)

arrows(-5, 0.16, -4.75, 0, length = 0.1)
arrows(5, 0.16, 4.75, 0, length = 0.1)

text(-3.5, 0.22, 
     labels = expression("0.025"~"="~over(alpha, 2)),
     cex = 0.7)
text(3.5, 0.22, 
     labels = expression("0.025"~"="~over(alpha, 2)),
     cex = 0.7)

text(-5, 0.18, 
     labels = expression(paste("-|",t[act],"|")), 
     cex = 0.7)
text(5, 0.18, 
     labels = expression(paste("|",t[act],"|")), 
     cex = 0.7)

# Add ticks indicating critical values at the 0.05-level, t^act and -t^act 
rug(c(-1.96, 1.96), ticksize  = 0.145, lwd = 2, col = "darkred")
rug(c(-tact, tact), ticksize  = -0.0451, lwd = 2, col = "darkgreen")
```

##### 신뢰구간
```{r}
# set seed for reproducibility
set.seed(4)

# generate and plot the sample data
Y <- rnorm(n = 100, 
           mean = 5, 
           sd = 5)

plot(Y, 
     pch = 19, 
     col = "steelblue")
```

* 신뢰구간 하한, 상한 구하기
```{r}
cbind(CIlower = mean(Y) - 1.96 * 5 / 10, CIupper = mean(Y) + 1.96 * 5 / 10)
```

```{r}
# set seed
set.seed(1)

# initialize vectors of lower and upper interval boundaries
lower <- numeric(10000)
upper <- numeric(10000)

# loop sampling / estimation / CI
for(i in 1:10000) {
  
  Y <- rnorm(100, mean = 5, sd = 5)
  lower[i] <- mean(Y) - 1.96 * 5 / 10
  upper[i] <- mean(Y) + 1.96 * 5 / 10
  
}

# join vectors of interval bounds in a matrix
CIs <- cbind(lower, upper)
```

```{r}
mean(CIs[, 1] <= 5 & 5 <= CIs[, 2])
```

##### 가변수 만들기
```{r}
CASchools$D <- CASchools$STR < 20
```

##### 가변수를 이용한 회귀분석
```{r}
dummy_model <- lm(score ~ D, data = CASchools)
summary(dummy_model)
```

##### 가변수 회귀분석에 대한 신뢰구간
```{r}
confint(dummy_model)
```


```{r}
# Plot the data
plot(CASchools$D, CASchools$score,            # provide the data to be plotted
     pch = 20,                                # use filled circles as plot symbols
     cex = 0.5,                               # set size of plot symbols to 0.5
     col = "Steelblue",                       # set the symbols' color to "Steelblue"
     xlab = expression(D[i]),                 # Set title and axis names
     ylab = "Test Score",
     main = "Dummy Regression")
```

### 5.2 회귀모형의 진단과 수정
#### 5.2.1 부분 F검정
* 이분산성(Heteroskedasticity) 이분산성은, 동분산성의 가정을 충족시키지 못하는 경우를 말한다. 동분산성의 가정이란, 회귀분석에서 오차항들의 분산이 일정하다는 가정이다
```{r}
library(scales)

set.seed(123) 
# set up vector of x coordinates
x <- rep(c(10, 15, 20, 25), each = 25)

# initialize vector of errors
e <- c()

# sample 100 errors such that the variance increases with x
e[1:25] <- rnorm(25, sd = 10)
e[26:50] <- rnorm(25, sd = 15)
e[51:75] <- rnorm(25, sd = 20)
e[76:100] <- rnorm(25, sd = 25)

# set up y
y <- 720 - 3.3 * x + e

# Estimate the model 
mod <- lm(y ~ x)

# Plot the data
plot(x = x, 
     y = y, 
     main = "An Example of Heteroskedasticity",
     xlab = "Student-Teacher Ratio",
     ylab = "Test Score",
     cex = 0.5, 
     pch = 19, 
     xlim = c(8, 27), 
     ylim = c(600, 710))

# Add the regression line to the plot
abline(mod, col = "darkred")

# Add boxplots to the plot
boxplot(formula = y ~ x, 
        add = TRUE, 
        at = c(10, 15, 20, 25), 
        col = alpha("gray", 0.4), 
        border = "black")
```

#### A Real-World Example for Heteroskedasticity
* 데이터에 summary를 할경우 각각의 기초통계량 제공
```{r}
library(AER)
data("CPSSWEducation")
attach(CPSSWEducation)

summary(CPSSWEducation)
```

* 그래프를 통해 이분산성 확인
  + summary() :  F-statistic: 441.9 on 1 and 2948 DF,  p-value: < 2.2e-16 이므로 이분상성을 띄고있다
```{r}
# estimate a simple regression model
labor_model <- lm(earnings ~ education)
summary(labor_model)
# plot observations and add the regression line
plot(education, 
     earnings, 
     ylim = c(0, 150))

abline(labor_model, 
       col = "steelblue", 
       lwd = 2)
```

##### 단순회귀 분석에서의 추론
* `earnings = -3.134 + 1.467*education`
  + education이 한 단위 증가 함에 따라 earnings은 1.467만큼 증가한다
```{r}
# print the contents of labor_model to the console
labor_model
```

##### 95% 신뢰구간
* 절편과 회귀계수에 대한 상한,하한 95% 신뢰구간
```{r}
confint(labor_model)
```

#### 5.2.2 변수변환

```{r}
library(AER)
library(stargazer)
```

```{r}
# prepare the data
library(AER)                                                     
data(CASchools)
CASchools$size <- CASchools$students/CASchools$teachers
CASchools$score <- (CASchools$read + CASchools$math) / 2       
```

```{r}
cor(CASchools$income, CASchools$score)
```

```{r}
# A General Strategy for Modelling Nonlinear Regression Functions
# fit a simple linear model
linear_model<- lm(score ~ income, data = CASchools)

# plot the observations
plot(CASchools$income, CASchools$score,
     col = "steelblue",
     pch = 20,
     xlab = "District Income (thousands of dollars)", 
     ylab = "Test Score",
     cex.main = 0.9,
     main = "Test Score vs. District Income and a Linear OLS Regression Function")

# add the regression line to the plot
abline(linear_model, 
       col = "red", 
       lwd = 2)

```

* income에 제곱항을 넣어 비선형모형 확인
```{r}
# fit the quadratic Model
quadratic_model <- lm(score ~ income + I(income^2), data = CASchools)

# obtain the model summary
coeftest(quadratic_model, vcov. = vcovHC, type = "HC1")
```

```{r}
# draw a scatterplot of the observations for income and test score
plot(CASchools$income, CASchools$score,
     col  = "steelblue",
     pch = 20,
     xlab = "District Income (thousands of dollars)",
     ylab = "Test Score",
     main = "Estimated Linear and Quadratic Regression Functions")

# add a linear function to the plot
abline(linear_model, col = "black", lwd = 2)

# add quatratic function to the plot
order_id <- order(CASchools$income)

lines(x = CASchools$income[order_id], 
      y = fitted(quadratic_model)[order_id],
      col = "red", 
      lwd = 2) 
```

#### Logarithms

```{r}
# estimate a level-log model
LinearLog_model <- lm(score ~ log(income), data = CASchools)

# compute robust summary
coeftest(LinearLog_model, 
         vcov = vcovHC, type = "HC1")
```

```{r}
# draw a scatterplot
plot(score ~ income, 
     col = "steelblue",
     pch = 20,
     data = CASchools,
     main = "Linear-Log Regression Line")

# add the linear-log regression line
order_id  <- order(CASchools$income)

lines(CASchools$income[order_id],
      fitted(LinearLog_model)[order_id], 
      col = "red", 
      lwd = 2)
```

#### 5.2.3 표준화 회귀계수
* 단위가 다를 경우 한단위 증가에 따른 표준화 된 값의 증가분을 알고싶을 때 
`scale()`함수를 사용
  + scale()을 사용할 경우 Estimate의 값이 클 수록 한단위 증가분에 대한 증가량이 크다는 것을 알 수 있다
```{r}
w = c(65,83,72,45,42,56,89,72,47,57,55)
h = c(160, 188,180,154,153,160,185,182,163,171,167)
age = c(12,24,27,11,14,15,19,30,15,17,16)

df = data.frame(age,h,w,stringsAsFactors = F)

l1 = lm(h~ age+w,df)
summary(l1)

l2 = lm(scale(h)~ scale(age)+scale(w),df)
summary(l2)
```



#### 5.2.4 다중공선성의 탐색
* 다중공선성, 통계학의 회귀분석에서 독립변수들 간에 강한 상관관계가 나타나는 문제이다. 독립변수들간에 정확한 선형관계가 존재하는 완전공선성의 경우와 독립변수들간에 높은 선형관계가 존재하는 다중공선성으로 구분하기도 한다. 이는 회귀분석의 전제 가정을 위배하는 것이므로 적절한 회귀분석을 위해 해결해야 하는 문제가 된다

* 다중공선성 문제가 있다면 독립변수와 종속변수의 관계가 양의 상관관계(+)에서 음의상관관계(-)로 잘못 판단 되어지는 경우가 있다

* 해결법
  + 상관관계가 높은 독립변수중 하나 혹은 일부를 제거
  + 변수를 변형시키거나 새로운 관측치를 이용
  + 자료를 수집하는 현장의 상황을 보아 상관관계의 이유를 파악하여 해결

* 판단 
  + vif : 분산팽창인자가 10상일 경우 다중공선성이 존재한다고 판단한다


##### 다중공선성 판단 
* 위 데이터 프레임에 대한 다중공선성 확인
  + vif값이 10보다 작으므로 다중공선성은 존재하지 않는다
  + 만약 vif값이 10보다 큰 변수가 있다면 그 변수를 제거 한 후 회귀모델을 다시 적용한다
```{r}
library(car) # vif 함수 사용하기 위한 라이브러리
vif(l1)
```

