---
title: "크롤링"
author: "hyo"
date: '2020. 04. 01 '
output: html_document
---
## 6. 크롤링
* 온라인에 존재하는 데이터가 증가함에 따라 크롤링을 통해 수집할 수 있는 데이터도 증가
* 통신구조 및 html/css에 대한 이해가 필요함
* html 형태의 경우 rvest, httr 패키지, JSON 형태의 경우 jsonlite를 통해 크롤링 가능
* 동적 웹사이트의 경우 Rselenium을 통해 크롤링 가능하지만 다소 복잡함 (http://henryquant.blogspot.com/2019/09/rselenium.html)

### 6.1 크롤링이란?
* 크롤링(Crawling)이란 사전적으로 기어다니는 것을 뜻함 
* Web상을 돌아다니면서 정보를 수집하는 행위

### 6.2 네이버 뉴스 크롤링하기
``` {r}
library(rvest)
library(stringr)
``` 
* 요즘 이슈화 된 코로나에 대한 뉴스를 크롤링해보자

##### 쿼리 이해하기
* 네이버 인터넷 창에 코로나를 검색 후 뉴스 탭을 선택하면 다음과 같은 화면이 나온다
* 화면에 나온 URL을 살펴보면 알수없는 영어와 기호의 조합으로 이루어져 있다
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic1.png) 

* 컴퓨터가 알아들을 수 있는 언어로 바꾸어주어야한다
  + iconv()함수를 통해 문자 인코딩을 바꾸어줌
  + paste함수를 사용하여 문자와 기호를 결합해줌
  + topupper()함수를 사용하여 소문자를 대문자로 만들어줌
``` {r}
query = "코로나"
query = iconv(query, to = 'UTF-8', toRaw = T)
query
unlist(query)
paste(unlist(query), collapse = '%')
# iconv(query, to = "UTF-8", toRaw = F)
query = paste0('%', paste(unlist(query), collapse = '%'))
query
query = toupper(query)
query
```

* 쿼리를 바꾸어주는 함수를 만들어줌
  + encoding_tr()를 통해 한줄 코드 작성 가능
``` {r}
encoding_tr = function(query){
   query = iconv(query, to = 'UTF-8', toRaw = T)
   # iconv(query, to = "UTF-8", toRaw = F)
   query = paste0('%', paste(unlist(query), collapse = '%'))
   query = toupper(query)
   return(query)   
}
encoding_tr("코로나")
``` 

#### 6.2.1 url 정리하기
* 쿼리 앞의 url과 인코딩 된 quary를 paste0을 사용하여 결합
  + 아래 출력된 url을 따라가보면 "코로나확진자"에 대한 뉴스가 나옴
  
``` {r}
naver_url = "https://search.naver.com/search.naver?where=news&sm=tab_jum&query="
quary = encoding_tr("코로나확진자")
naver_url_news = paste0(naver_url,quary)
naver_url_news
```
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic2.png) 

#### 6.2.2 url 연결하기
* F12를 누르면 다음과 같은 화면이 나온다
* 보이는 화면은 html언어로 쓰여져 있음
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic3.png)

##### `read_html()` 함수를 사용하여 url의 html을 확인 가능
``` {r}
html = read_html(naver_url_news,encoding = "utf-8")
html
``` 

#### 6.2.3 네이버 뉴스 기사 제목 크롤링
##### `html_nodes(),html_text()` 함수를 사용
* ctrl+B를 누른 뒤 마우스로 크롤링하고 싶은 부분을 클릭 한다
  + 아래 화면 참고
  ![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic4.png) 

* html_nodes() : 해당 노드를 탐색 후 grep
  + * class는 . 으로 표현
  ![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic5.png)  
* html_text()  : 특수문자 등을 제거 후 text만 갖고 오기

``` {r}
html %>% html_nodes("div.title_desc") %>% html_text()
```
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic6.png)  

``` {r}
html2 = html %>% html_nodes("dt") %>% html_nodes("a") %>% html_text()
html2
``` 

#### 6.2.4 text로 글 정제하기
* 정규 표현식을 활용하여 특수 문자를 제거한다
  + [0-9] : 숫자 제거
  + [a-zA-Z] : 알파벳 제거
  + [[:cntrl:]]|[[:punct:]] : 특수문자 제거
* 필요한 데이터만 저장
  + working directory를 확인 해보면 csv파일로 저장이 되어있는 것을 확인 할 수 있다

``` {r}
tmp = gsub("[0-9]|[a-zA-Z]","",html2[2:11])
tmp
```

``` {r}
tmp = gsub("[[:cntrl:]]|[[:punct:]]","",html2[2:11])
tmp
covid19 = data.frame(tmp,stringsAsFactors = F)
write.csv(covid19,"covid19.csv",row.names = F)
``` 

#### 6.2.5 for문을 활용하여 년도월별 추출
* "코로나키트"에 대한 키워드로 2020년 3월 27일 ~ 2020년 3월 28일 뉴스 제목 크롤링 
  + 검색 옵션에서 기간을 선택
  + 여러 page를 선택하기 위해 page 2 선택하여 url 확인
  + url을 분석하여 변하는 부분을 매개변수로 대체
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic7.png)  
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/pic8.png)  

* dataframe, rbind를 활용하여 1~10page 각 10개씩 100개의 뉴스제목 크롤링
``` {r}
subject = "코로나키트"
query = encoding_tr(subject)

from1 = "2020.03.27" 
to1 = "2020.03.28"

from2 = gsub("\\.","",from1)
to2 = gsub("\\.","",to1)

f = data.frame(stringsAsFactors = F)
n = 1
for(i in 1:10){
   url = paste0("https://search.naver.com/search.naver?&where=news&query=",query,"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=",from1,"&de=",to1,"&docid=&nso=so:r,p:from",from2,"to",to2,",a:all&mynews=0&start=",n,"&refresh_start=0")
   html = read_html(url,encoding = "utf-8")
   html2 = html %>% html_nodes("dt") %>% html_nodes("a") %>% html_text()
   tmp = data.frame(html2[2:11],stringsAsFactors = F)
   f = rbind(f,tmp)
   n = n + 10
}
head(f)
tail(f)
names(f) = subject

write.csv(f,"covid19kit.csv",row.names = F)
```

### 6.3 네이버 블로그 크롤링하기  
#### 6.3.1 api활용
##### open api 발급 받는 법
* [네이버 개발자 센터] -> [서비스 api] -> [오픈 api 신청] -> [로그인] -> [url(자신의 블로그주소)]
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog1.png) 
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog2.png) 
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog3.png) 
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog4.png) 
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog5.png) 

![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog6.png) 


* client_id,client_secret 아래 코드에 복사 붙혀 넣기
   + 네이버 정책에 따라 발급 open api 인증 시간이 걸릴 수 있음
``` {r}
client_id = '?????????'      # 1. 위 발급 client_id
client_secret = '?????????'  # 2. 위 발급 client_secret
header = httr::add_headers(
   'X-Naver-Client-Id' = client_id,
   'X-Naver-Client-Secret' = client_secret)
```

``` {r,echo =FALSE}
# client_id, client_secret은 수업자료에는 '???'로 표기 되어 있습니다.
client_id = 'FrKq4uDK2LexR1_Cfx6Q'
client_secret = 'AV7LBwCv0d'
header = httr::add_headers(
   'X-Naver-Client-Id' = client_id,
   'X-Naver-Client-Secret' = client_secret)
```


#### 6.3.2 네이버 블로그 내용 크롤링
* 아래 사이트를 참고 하면 개발 가이드를 볼 수 있음
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog7.png) 
![](C:/Users/haley/Desktop/haley/task/Rbasic/statistic_lecture/크롤링/blog8.png) 

* 뉴스 크롤링에서 사용 한 함수 encoding_tr 사용하여 쿼리 변환 
``` {r}
query = encoding_tr("서울맛집")
```

``` {r}
library(httr)
library(xml2)
library(rvest)
end_num = 1000
display_num = 100
start_point = seq(1,end_num,display_num)
i = 1
url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',
             query,'&display=',display_num,'&start=',
             start_point[i],'&sort=sim')
url_body = read_xml(GET(url, header))
```

##### `xml_nodes() xml_text()`함수 이용
* xml_nodes() : 정보를 이어 주는 node
   + title : "item title"
   + bloggername : "item bloggername"
   + link : "item link"
   + description : "item description"
* xml_text()  : xml언어로 이루어진 글들 중 원하는 정보만을 text화 시킴

``` {r}
# --------------------
title = url_body %>% xml_nodes('item title') %>%
   xml_text()
bloggername = url_body %>% 
   xml_nodes('item bloggername') %>% xml_text()
postdate = url_body %>% xml_nodes('postdate') %>%
   xml_text()
link = url_body %>% xml_nodes('item link') %>%
   xml_text()
description = url_body %>% xml_nodes('item description') %>%
   html_text()
```

##### for문을 사용하여 여러개의 블로그를 크롤링
``` {r}
i = 1
final_dat = NULL
for(i in 1:length(start_point)){
   # request xml format
   url = paste0('https://openapi.naver.com/v1/search/blog.xml?query=',query,'&display=',display_num,'&start=',start_point[i],'&sort=sim')
   #option header
   url_body = read_xml(GET(url, header), encoding = "UTF-8")
   title = url_body %>% xml_nodes('item title') %>% xml_text()
   bloggername = url_body %>% xml_nodes('item bloggername') %>% xml_text()
   postdate = url_body %>% xml_nodes('postdate') %>% xml_text()
   link = url_body %>% xml_nodes('item link') %>% xml_text()
   description = url_body %>% xml_nodes('item description') %>% html_text()
   temp_dat = cbind(title, bloggername, postdate, link, description)
   final_dat = rbind(final_dat, temp_dat)
   #cat(i, '\n')
}
final_dat = data.frame(final_dat, stringsAsFactors = F)
final_dat$postdate=as.Date(final_dat$postdate,format="%Y%m%d")
write.csv(final_dat,"맛집.csv")
```

``` {r}
head(final_dat)
names(final_dat)
```

##### 크롤링한 데이터 분포 확인
``` {r}
tb=table(final_dat$postdate)
plot(tb)
```

#### 6.3.3 text로 글 정제하기
* 정규 표현식을 활용하여 특수 문자를 제거한다
  + [^가-힣( )] : 한글과 공백이 아닌 다른 문자
  + [[:cntrl:]]|[[:punct:]] : 특수문자 제거
* 필요한 데이터만 저장
  + working directory를 확인 해보면 csv파일로 저장이 되어있는 것을 확인 할 수 있다

``` {r}
tmp = gsub("[^가-힣( )]","",final_dat$description)
head(tmp)
```

``` {r}
tmp = gsub("[[:cntrl:]]|[[:punct:]]","",tmp)
head(tmp)
food_review = data.frame(tmp,stringsAsFactors = F)
write.csv(covid19,"food_review.csv",row.names = F)
``` 
### 6.4 httr 라이브러리 사용 크롤링 예제
* KODEX 200의 투자종목정보(excel)를 다운받는 예제
* http://www.kodex.com/product_view.do?fId=2ETF01

``` {r}
library(httr)
library(rvest)
library(readxl)

pdf_200 = POST(url = 'http://www.kodex.com/excel_pdf.do',
               query = list(
                 fId = '2ETF01',
                 gijunYMD = 20191219
               ),
               write_disk(tf <- tempfile(fileext = '.xls')))
pdf_data = read_excel(tf, skip = 2, col_names = TRUE)

print(pdf_data)
```


* 금융 속보의 제목 크롤링
* 크롬의 개발자도구 화면 이용해 HTML 분해
* https://finance.naver.com/news/news_list.nhn?mode=LSS2D&section_id=101&section_id2=258

``` {r}
url = 'https://finance.naver.com/news/news_list.nhn?mode=LSS2D&section_id=101&section_id2=258'
data = GET(url)
print(data)
```



``` {r}
data_title = data %>%
  read_html(encoding = 'EUC-KR') %>%
  html_nodes('dl') %>%
  html_nodes('.articleSubject') %>%
  html_nodes('a') %>%
  html_attr('title')

print(data_title)
```

